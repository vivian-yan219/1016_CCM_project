# 1016_CCM_project: Evaluating LLM Question Asking Using Information-Theoretic Measures

## Project Setting
This project aims to evaluate the ability of Large Language Models (LLMs) to ask high-quality questions using metrics derived from information theory. Building on the approach from (Rothe, Lake, & Gureckis, 2018) that explores human question-asking through a Bayesian ideal observer model, we propose a study on LLMs capability of asking questions strategically to complete the assigned task. We will be using the same game based task, which is the modified battleship game used in the original paper, to measure the quality of questions the LLM asks, allowing for comparison of the questions generated by humans and LLMs.

## Evaluation
The evaluation of LLM-generated questions will be based on Expected Information Gain (EIG), which quantifies how much uncertainty is reduced by each question. LLM performance will be compared to human participants to assess whether LLMs can strategically acquire information similarly to humans or even outperform humans. The LLMs will be prompted to ask questions in a free-form manner, with their outputs evaluated against human benchmarks and computational models. It is mentioned in the previous study (Rothe et al., 2018) that computational demands in human cognition may limit the ability to ask the best questions in real-time situations. We plan to explore how the cumputational demand differs between an LLM and human cognition.
Based upon the initial evaluation of LLM’s capability of strategically asking questions, this study will also explore ways to enhance model’s performance to ask better questions and further optimize information gain. For this part of experiment, we propose two methods:
1. Fine-tune model parameters.
2. Leverage a Chain-of-Thoughts (CoT) as an approach that utilizes LLM’s earlier output as short-term memory to bias the LLM output towards that better performance.

## Summary
By combining cognitive modeling techniques with contemporary machine learning models, this project aims to explore the effectiveness of LLMs in active learning and question-asking tasks, providing valuable insights into how LLMs can be trained to adopt more strategic approaches in acquiring information.
